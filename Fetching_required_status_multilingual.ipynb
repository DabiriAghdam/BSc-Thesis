{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDsBxUgeChew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "536c2439-3706-4adc-fce6-8b41377fd19e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets #==1.9.0\n",
        "!pip install -q transformers #==4.10.3\n",
        "import transformers\n",
        "import torch\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import savetxt, loadtxt\n",
        "import os\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set configuration"
      ],
      "metadata": {
        "id": "qtewqlGotooU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Fine_tune_func():\n",
        "  ftn = int(input(f\"please enter 1 if you are in the fine-tune mode, otherwise enter 0: \"))\n",
        "  while ftn not in [0,1]:\n",
        "    print(f\"invalid input, try again!\")\n",
        "    ftn = int(input(f\"please enter 1 if you are in the fine-tune mode, otherwise enter 0: \"))\n",
        "  return ftn"
      ],
      "metadata": {
        "id": "n6BQT-yHtjYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_for_Modes():\n",
        "  '''\n",
        "  This function is explicitly designed for single-lingual mode, and will be used in the time of needs.\n",
        "  '''\n",
        "  # Receiving the Mode (Single/Multi lingual)\n",
        "  Mode = int(input(f\"Enter 1 if you are surfing on the single-lingual mode, otherwise consider this as irrelevent by entering 0: \"))\n",
        "  while Mode not in [0,1]:\n",
        "    print(f\"invalid input, try again!\")\n",
        "    Mode = int(input(f\"Enter 1 if you are surfing on the single-lingual mode, otherwise consider this as irrelevent by entering 0: \"))\n",
        "  if Mode:\n",
        "    # Receiving the code per lingual.\n",
        "    code = int(input(f\"please enter 0 for (French), enter 1 for (English), and 2 for (Farsi): \"))\n",
        "    while code not in [0,1,2]:\n",
        "      print(f\"invalid input, try again!\")\n",
        "      code = int(input(f\"please enter 0 for (French), enter 1 for (English), and 2 for (Farsi): \"))\n",
        "    return code\n",
        "  else:\n",
        "    return \"Multi-lingual\""
      ],
      "metadata": {
        "id": "lFy9hpNCtkSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Combination_def():\n",
        "  comb = int(input(f\"please enter 0 for (French & English), 1 for (English & Farsi), and 2 for (French & Farsi): \"))\n",
        "  while comb not in [0,1,2]:\n",
        "    print(f\"invalid input, try again!\")\n",
        "    comb = int(input(f\"please enter 0 for (French & English), 1 for (English & Farsi), and 2 for (French & Farsi): \"))\n",
        "\n",
        "  if comb == 0:\n",
        "    dataset_code = [0,1] # French & English\n",
        "  elif comb == 1:\n",
        "    dataset_code = [1,2] # English & Farsi\n",
        "  else:\n",
        "    dataset_code = [0,2] # French & Farsi\n",
        "\n",
        "  return comb, dataset_code"
      ],
      "metadata": {
        "id": "BMAflaeVtnpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_path(Fine_tune, Combination, dataset_code, from_gdrive=True):\n",
        "\n",
        "  if (from_gdrive):\n",
        "    Actual_path = \"/content/drive/Shareddrives/Gdrive/NLP Bachelors' Project/checkpoint/\"\n",
        "  else:\n",
        "    Actual_path = \"/content/drive/MyDrive/NLP Bachelors' Project/checkpoint/\"\n",
        "\n",
        "\n",
        "  if not isinstance(dataset_code, int):\n",
        "\n",
        "    Normal_path = Actual_path + \"MLL\"\n",
        "    Fine_tune_path = Actual_path + \"Fine-tune/MLL\"\n",
        "\n",
        "    if Fine_tune:\n",
        "        if Combination == 0:\n",
        "\n",
        "            path_to_checkpoint = Fine_tune_path + \"(English,French)\"\n",
        "            path_to_pretrained_weight = Normal_path + \"(English,French)\"\n",
        "\n",
        "        elif Combination == 1:\n",
        "\n",
        "            path_to_checkpoint = Fine_tune_path + \"(Farsi,English)\"\n",
        "            path_to_pretrained_weight = Normal_path + \"(Farsi,English)\"\n",
        "\n",
        "        elif Combination == 2:\n",
        "\n",
        "            path_to_checkpoint = Fine_tune_path + \"(French,Farsi)\"\n",
        "            path_to_pretrained_weight = Normal_path + \"(French,Farsi)\"\n",
        "\n",
        "    else:\n",
        "        path_to_pretrained_weight = None\n",
        "        if Combination == 0:\n",
        "\n",
        "            path_to_checkpoint = Normal_path + \"(English,French)\"\n",
        "\n",
        "        elif Combination == 1:\n",
        "\n",
        "            path_to_checkpoint = Normal_path + \"(Farsi,English)\"\n",
        "\n",
        "        elif Combination == 2:\n",
        "\n",
        "            path_to_checkpoint = Normal_path + \"(French,Farsi)\"\n",
        "\n",
        "  else:\n",
        "    if (Fine_tune):\n",
        "        Normal_path = Actual_path + \"SLL\"\n",
        "        Fine_tune_path = Actual_path + \"Fine-tune/SLL\"\n",
        "        if dataset_code == 0:\n",
        "            path_to_checkpoint = Fine_tune_path + \"(French)\"\n",
        "            path_to_pretrained_weight = Normal_path + \"(French)\"\n",
        "        elif dataset_code == 1:\n",
        "            path_to_checkpoint = Fine_tune_path + \"(English)\"\n",
        "            path_to_pretrained_weight = Normal_path + \"(English)\"\n",
        "        else:\n",
        "            path_to_checkpoint = Fine_tune_path + \"(Farsi)\"\n",
        "            path_to_pretrained_weight = Normal_path + \"(Farsi)\"\n",
        "    else:\n",
        "        Normal_path = Actual_path + \"SLL\"\n",
        "        path_to_pretrained_weight = None\n",
        "        if dataset_code == 0:\n",
        "            path_to_checkpoint = Normal_path + \"(French)\"\n",
        "        elif dataset_code == 1:\n",
        "            path_to_checkpoint = Normal_path + \"(English)\"\n",
        "        else:\n",
        "            path_to_checkpoint = Normal_path + \"(Farsi)\"\n",
        "\n",
        "  return path_to_checkpoint, path_to_pretrained_weight"
      ],
      "metadata": {
        "id": "nM6YtV1Htxsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_status():\n",
        "  Fine_tune = Fine_tune_func()\n",
        "  #Combination = None\n",
        "  Combination, dataset_code = Combination_def() # dataset_code in multi_lingual mode is list of two items, whereas in single-lingual mode we have a single integer.\n",
        "\n",
        "  return Fine_tune, Combination, dataset_code\n",
        ""
      ],
      "metadata": {
        "id": "rHfVuwnstz1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def return_last_checkpoint(path):\n",
        "#   if (path and len(os.listdir(path)) > 0):\n",
        "#     lst = os.listdir(path)\n",
        "#     lst = [item for item in lst if item[:10]=='checkpoint']\n",
        "#     num = [item.split('checkpoint-') for item in lst]\n",
        "#     return lst[num.index(max(num))]\n",
        "#   else:\n",
        "#     return None"
      ],
      "metadata": {
        "id": "PCtbNELtyt0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_last_checkpoint(path):\n",
        "  if (path and len(os.listdir(path)) > 0):\n",
        "    lst = os.listdir(path)\n",
        "    lst = [item for item in lst if item[:10]=='checkpoint']\n",
        "    num = [int(item[11:]) for item in lst]\n",
        "    return lst[num.index(max(num))]\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "ahVzpHrQmtsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def return_model_name(Fine_tune, path, Last_checkpoint):\n",
        "#   if Fine_tune == 0:\n",
        "#     model_name = \"xlm-roberta-base\"\n",
        "#   else:\n",
        "#     ################### for test #########################\n",
        "#     if (Last_checkpoint): model_name = path + \"/\" + Last_checkpoint\n",
        "#     else: model_name = path + \"/\"\n",
        "#     ################ end ######################\n",
        "#     json_filename = model_name + \"/config.json\"\n",
        "#     with open(json_filename) as json_file:\n",
        "#       json_decoded = json.load(json_file)\n",
        "\n",
        "#     json_decoded['model_type'] = 'xlm-roberta' #??\n",
        "\n",
        "#     json_decoded['attention_probs_dropout_prob'] = 0.1 #??\n",
        "#     json_decoded['bos_token_id'] = 0 #??\n",
        "#     json_decoded['classifier_dropout'] = None #??\n",
        "#     json_decoded['eos_token_id'] = 2 #??\n",
        "#     json_decoded['hidden_act'] = \"gelu\" #??\n",
        "#     json_decoded['hidden_dropout_prob'] = 0.1 #??\n",
        "#     json_decoded['hidden_size'] = 768 #??\n",
        "#     json_decoded['initializer_range'] = 0.02 #??\n",
        "#     json_decoded['intermediate_size'] = 3072 #??\n",
        "#     json_decoded['layer_norm_eps'] = 1e-05 #??\n",
        "#     json_decoded['max_position_embeddings'] = 514 #??\n",
        "#     json_decoded['num_attention_heads'] = 12 #??\n",
        "#     json_decoded['num_hidden_layers'] = 12 #??\n",
        "#     json_decoded['output_past'] = True #??\n",
        "#     json_decoded['pad_token_id'] = 1 #??\n",
        "#     json_decoded['position_embedding_type'] = \"absolute\" #??\n",
        "#     json_decoded['type_vocab_size'] = 1 #??\n",
        "#     json_decoded['use_cache'] = True #??\n",
        "#     json_decoded['vocab_size'] = 250002 #??\n",
        "\n",
        "#     with open(json_filename, 'w') as json_file:\n",
        "#       json.dump(json_decoded, json_file, indent=2, separators=(',', ': '))\n",
        "\n",
        "#   return model_name"
      ],
      "metadata": {
        "id": "z1OEYr03E0wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_model_name(Fine_tune, path, Last_checkpoint):\n",
        "    model_name = \"xlm-roberta-base\"\n",
        "    return model_name\n",
        "  # if Fine_tune == 0:\n",
        "  #   model_name = \"bert-base-uncased\"\n",
        "  # else:\n",
        "  #   model_name = path + \"/\" + Last_checkpoint\n",
        "  #   json_filename = model_name + \"/config.json\"\n",
        "  #   with open(json_filename) as json_file:\n",
        "  #     json_decoded = json.load(json_file)\n",
        "\n",
        "  #   json_decoded['model_type'] = 'bert'\n",
        "\n",
        "  #   with open(json_filename, 'w') as json_file:\n",
        "  #     json.dump(json_decoded, json_file, indent=2, separators=(',', ': '))\n",
        "\n",
        "  # return model_name"
      ],
      "metadata": {
        "id": "3tq_oush5fHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset setup"
      ],
      "metadata": {
        "id": "Du7keCSottIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dict():\n",
        "    import pickle\n",
        "    dataset = load_dataset(\"AmazonScience/massive\", name='en-US')\n",
        "    dataset = load_dataset(\"AmazonScience/massive\", name='fa-IR')\n",
        "    dataset = load_dataset(\"AmazonScience/massive\", name='fr-FR')\n",
        "    with open(f\"/content/drive/MyDrive/NLP Bachelors' Project/en_dataset.pkl\", 'rb') as f:\n",
        "        massive_en = pickle.load(f)\n",
        "\n",
        "    with open(f\"/content/drive/MyDrive/NLP Bachelors' Project/fa_dataset.pkl\", 'rb') as f:\n",
        "        massive_fa = pickle.load(f)\n",
        "\n",
        "    with open(f\"/content/drive/MyDrive/NLP Bachelors' Project/fr_dataset.pkl\", 'rb') as f:\n",
        "        massive_fr = pickle.load(f)\n",
        "\n",
        "    dataset_dict = {\n",
        "    \"French\": massive_fr,\n",
        "    \"English\": massive_en,\n",
        "    \"Farsi\": massive_fa,\n",
        "  }\n",
        "    return dataset_dict\n"
      ],
      "metadata": {
        "id": "VyNYZE1n6BaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_prepro_Entailment(dataset_dict):\n",
        "#   df_train_Entailment = pd.DataFrame(dataset_dict[\"Entailment\"]['train'])\n",
        "#   df_test_Entailment = pd.DataFrame(dataset_dict[\"Entailment\"]['test'])\n",
        "#   df_validation_Entailment =  pd.DataFrame(dataset_dict[\"Entailment\"]['validation'])\n",
        "#   df_train_Entailment['label'] = (df_train_Entailment['label'] == 'entails').astype(int)\n",
        "#   df_test_Entailment['label'] = (df_test_Entailment['label'] == 'entails').astype(int)\n",
        "#   df_validation_Entailment['label'] = (df_validation_Entailment['label'] == 'entails').astype(int)\n",
        "#   dataset_dict[\"Entailment\"]['train'] = Dataset.from_pandas(df_train_Entailment)\n",
        "#   dataset_dict[\"Entailment\"]['test'] = Dataset.from_pandas(df_test_Entailment)\n",
        "#   dataset_dict[\"Entailment\"]['validation'] = Dataset.from_pandas(df_validation_Entailment)\n",
        "\n",
        "#   return dataset_dict[\"Entailment\"]"
      ],
      "metadata": {
        "id": "U5K45GgN67ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_directory_status(\n",
        "    dataset_dict,\n",
        "    resume_from_checkpoint,\n",
        "    path_to_checkpoint,\n",
        "    Combination,\n",
        "    Dataset_code,\n",
        "    num_epochs,\n",
        "    Saved_prop_task1: None,\n",
        "    Saved_prop_task2: None,\n",
        "    Mode = 1):\n",
        "\n",
        "  '''\n",
        "  Here we check the status of our directory in terms of having .csv files incorporating the probablities of the saved epochs.\n",
        "  As we want to continue from the last saved epoch, we have to save the probabilities as well (starting from the last saved row in our numpy array)\n",
        "  -------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "  Mode is a flag to save or load the array of probabilities. If Mode == 1, we are in the loading mode, otherwise we are in the saving mode.\n",
        "  '''\n",
        "  if isinstance(Dataset_code, list):\n",
        "\n",
        "    if Combination == 0:\n",
        "        # paths_to_check = [os.path.join(path_to_checkpoint, 'Prob_per_epoch_French.csv')] + [os.path.join(path_to_checkpoint, 'Prob_per_epoch_English.csv')]\n",
        "        paths_to_check = [os.path.join(path_to_checkpoint, 'Prob_per_epoch_English.csv')] + [os.path.join(path_to_checkpoint, 'Prob_per_epoch_French.csv')] #??\n",
        "    elif Combination == 1:\n",
        "        paths_to_check = [os.path.join(path_to_checkpoint, 'Prob_per_epoch_English.csv')] + [os.path.join(path_to_checkpoint, 'Prob_per_epoch_Farsi.csv')]\n",
        "    else:\n",
        "        paths_to_check = [os.path.join(path_to_checkpoint, 'Prob_per_epoch_French.csv')] + [os.path.join(path_to_checkpoint, 'Prob_per_epoch_Farsi.csv')]\n",
        "\n",
        "\n",
        "    if Mode:\n",
        "\n",
        "        if os.path.exists(paths_to_check[0]) and resume_from_checkpoint is not None:  # Checking for one of  the paths reltaed to each multi lingual mode would be enough.\n",
        "            Prob_per_epoch_first_task = loadtxt(paths_to_check[0], delimiter=',')\n",
        "            Prob_per_epoch_second_task = loadtxt(paths_to_check[1], delimiter=',')\n",
        "\n",
        "            Prob_per_epoch_first_task = Prob_per_epoch_first_task.reshape(Prob_per_epoch_first_task.shape[0], Prob_per_epoch_first_task.shape[1] // 60, 60)\n",
        "            Prob_per_epoch_second_task = Prob_per_epoch_second_task.reshape(Prob_per_epoch_second_task.shape[0], Prob_per_epoch_second_task.shape[1] // 60, 60)\n",
        "\n",
        "        else:\n",
        "            Prob_per_epoch_first_task = np.zeros((num_epochs,len(dataset_dict[list(dataset_dict.keys())[0]]['train']), 60)) #60 labels\n",
        "            Prob_per_epoch_second_task = np.zeros((num_epochs,len(dataset_dict[list(dataset_dict.keys())[1]]['train']), 60)) #60 labels\n",
        "\n",
        "        return Prob_per_epoch_first_task, Prob_per_epoch_second_task\n",
        "\n",
        "    else:\n",
        "\n",
        "        savetxt(paths_to_check[0], Saved_prop_task1.reshape(Saved_prop_task1.shape[0], -1), delimiter=',')\n",
        "        savetxt(paths_to_check[1], Saved_prop_task2.reshape(Saved_prop_task2.shape[0],-1), delimiter=',')\n",
        "    # print(\"saving to: \", paths_to_check)\n",
        "    # Prob_per_epoch_Paraphrase = np.zeros((NUM_EPOCHS,len(dataset_dict[\"mrpc\"]['train']),OUTPUT_DIM)) #2 labels\n",
        "\n",
        "  else:\n",
        "\n",
        "    if Dataset_code == 0:\n",
        "      paths_to_check = [os.path.join(path_to_checkpoint, 'Prob_per_epoch_French.csv')]\n",
        "    elif Dataset_code == 1:\n",
        "      paths_to_check = [os.path.join(path_to_checkpoint, 'Prob_per_epoch_English.csv')]\n",
        "    else:\n",
        "      paths_to_check = [os.path.join(path_to_checkpoint, 'Prob_per_epoch_Farsi.csv')]\n",
        "\n",
        "\n",
        "  if Mode:\n",
        "\n",
        "    if os.path.exists(paths_to_check[0]) and resume_from_checkpoint is not None:  # Checking for one of the paths associated with each multi task mode would be enough.\n",
        "\n",
        "      if isinstance(Dataset_code, list):\n",
        "\n",
        "        Prob_per_epoch_first_task = loadtxt(paths_to_check[0], delimiter=',')\n",
        "        Prob_per_epoch_second_task = loadtxt(paths_to_check[1], delimiter=',')\n",
        "\n",
        "        Prob_per_epoch_first_task = Prob_per_epoch_first_task.reshape(Prob_per_epoch_first_task.shape[0], Prob_per_epoch_first_task.shape[1] // 60, 60)\n",
        "        Prob_per_epoch_second_task = Prob_per_epoch_second_task.reshape(Prob_per_epoch_second_task.shape[0], Prob_per_epoch_second_task.shape[1] //  60,  60)\n",
        "\n",
        "      else:\n",
        "\n",
        "        Prob_per_epoch_first_task = loadtxt(paths_to_check[0], delimiter=',')\n",
        "\n",
        "        Prob_per_epoch_first_task = Prob_per_epoch_first_task.reshape(Prob_per_epoch_first_task.shape[0], Prob_per_epoch_first_task.shape[1] // 60, 60)\n",
        "        Prob_per_epoch_second_task =  None\n",
        "\n",
        "    else:\n",
        "\n",
        "      if isinstance(Dataset_code, list):\n",
        "\n",
        "        Prob_per_epoch_first_task = np.zeros((num_epochs, len(dataset_dict[list(dataset_dict.keys())[0]]['train']), 60)) #60 labels\n",
        "        Prob_per_epoch_second_task = np.zeros((num_epochs, len(dataset_dict[list(dataset_dict.keys())[1]]['train']), 60)) #60 labels\n",
        "\n",
        "      else:\n",
        "\n",
        "        Prob_per_epoch_first_task = np.zeros((num_epochs, len(dataset_dict[list(dataset_dict.keys())[0]]['train']), 60)) #60 labels\n",
        "        Prob_per_epoch_second_task = None\n",
        "\n",
        "\n",
        "    return Prob_per_epoch_first_task, Prob_per_epoch_second_task\n",
        "\n",
        "  else:\n",
        "\n",
        "    if isinstance(Dataset_code, list):\n",
        "      savetxt(paths_to_check[0], Saved_prop_task1.reshape(Saved_prop_task1.shape[0], -1), delimiter=',')\n",
        "      savetxt(paths_to_check[1], Saved_prop_task2.reshape(Saved_prop_task2.shape[0],-1), delimiter=',')\n",
        "    else:\n",
        "      savetxt(paths_to_check[0], Saved_prop_task1.reshape(Saved_prop_task1.shape[0], -1), delimiter=',')\n",
        "    # Prob_per_epoch_Paraphrase = np.zeros((NUM_EPOCHS,len(dataset_dict[\"mrpc\"]['train']),OUTPUT_DIM)) #2 labels"
      ],
      "metadata": {
        "id": "QbzJNJNta00n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_FRG_path(dataset_code, Fine_tune, Combination, dfs = None, Test = 0, Mode = 1, Actual_path = \"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(pkl files)\", FRG_str = \"prob_per_10epochs\"):\n",
        "  '''\n",
        "  dataset_code is dependant on the type of learning procedure (Single / Multi lingual); if we are in a single lingual mode, dataset_code is a single integer ranging from 0 to 2,\n",
        "  otherwise, meaning we are in the multi-lingual mode, the dataset_code is a list consisting of two integers ranging from 0 to 2.\n",
        "  -------------------------------------------------------------------------------------------------------------------------------\n",
        "  Mode is set to switch between saving or loading mode, if Mode == 1, we are in the loading mode, otherwise, we are in the saving mode.\n",
        "  '''\n",
        "  task_name = ['French_', \"English_\", \"Farsi_\"]\n",
        "  post_MTL_str = [\"_FREN\", \"_ENFA\", \"_FRFA\"]\n",
        "  prob = [None, None]\n",
        "  if isinstance(dataset_code, list): # we are in the multi-lingual mode.\n",
        "    Selected_path = (os.path.join(Actual_path, \"Multi Lingual/\" + (\"Fine-tune/\" if Fine_tune else \"\") + task_name[dataset_code[0]] + \"MLL_\" + (FRG_str if Mode else \"FRG_Info\") + post_MTL_str[Combination] + (\"_Test\" if Test else \"\") + (\".pkl\" if Mode else \".csv\")),\n",
        "                    os.path.join(Actual_path, \"Multi Lingual/\" + (\"Fine-tune/\" if Fine_tune else \"\") + task_name[dataset_code[1]] + \"MLL_\" + (FRG_str if Mode else \"FRG_Info\") + post_MTL_str[Combination] + (\"_Test\" if Test else \"\") + (\".pkl\" if Mode else \".csv\")))\n",
        "\n",
        "    if Mode:\n",
        "      with open(Selected_path[0],'rb') as f1: prob[0] = pickle.load(f1)\n",
        "      with open(Selected_path[1],'rb') as f2: prob[1] = pickle.load(f2)\n",
        "    else:\n",
        "      dfs[0].to_csv(Selected_path[0])\n",
        "      dfs[1].to_csv(Selected_path[1])\n",
        "\n",
        "  else: # we are in the single-lingual mode.\n",
        "    Selected_path = os.path.join(Actual_path, \"Single Lingual/\" + (\"Fine-tune/\" if Fine_tune else \"\") + task_name[dataset_code] + (FRG_str if Mode else \"FRG_Info\") + (\".pkl\" if Mode else \".csv\"))\n",
        "    if Mode:\n",
        "      with open(Selected_path,'rb') as f: prob = pickle.load(f)\n",
        "    else:\n",
        "      dfs.to_csv(Selected_path)\n",
        "\n",
        "  return prob\n"
      ],
      "metadata": {
        "id": "uQeoSdDf-9Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_final_prop(Dataset_code, Fine_tune, Combination, prob_first_task, prob_second_task):\n",
        "\n",
        "  Actual_path = \"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(pkl files)/Multi Lingual\"\n",
        "  Fine_tune_path = \"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(pkl files)/Multi Lingual/Fine-tune\"\n",
        "\n",
        "  if isinstance(Dataset_code, list):\n",
        "\n",
        "    if Fine_tune:\n",
        "        Selected_path = Fine_tune_path\n",
        "    else:\n",
        "        Selected_path = Actual_path\n",
        "\n",
        "    if Combination == 0:\n",
        "\n",
        "        # first_path = os.path.join(Selected_path, \"French_\" + \"MLL_prob_per_10epochs_\" + \"FREN\" + \".pkl\")\n",
        "        # second_path = os.path.join(Selected_path, \"English_\" + \"MLL_prob_per_10epochs_\" + \"FREN\" + \".pkl\")\n",
        "\n",
        "        first_path = os.path.join(Selected_path, \"English_\" + \"MLL_prob_per_10epochs_\" + \"FREN\" + \".pkl\")\n",
        "        second_path = os.path.join(Selected_path, \"French_\" + \"MLL_prob_per_10epochs_\" + \"FREN\" + \".pkl\")\n",
        "\n",
        "    elif Combination == 1:\n",
        "\n",
        "        first_path = os.path.join(Selected_path, \"English_\" + \"MLL_prob_per_10epochs_\" + \"ENFA\" + \".pkl\")\n",
        "        second_path = os.path.join(Selected_path, \"Farsi_\" + \"MLL_prob_per_10epochs_\" + \"ENFA\" + \".pkl\")\n",
        "\n",
        "    else:\n",
        "\n",
        "        first_path = os.path.join(Selected_path, \"French_\" + \"MLL_prob_per_10epochs_\" + \"FRFA\" + \".pkl\")\n",
        "        second_path = os.path.join(Selected_path, \"Farsi_\" + \"MLL_prob_per_10epochs_\" + \"FRFA\" + \".pkl\")\n",
        "\n",
        "\n",
        "    with open(first_path, 'wb') as f:\n",
        "\n",
        "        pickle.dump(prob_first_task, f)\n",
        "\n",
        "    with open(second_path,'wb') as f:\n",
        "\n",
        "        pickle.dump(prob_second_task, f)\n",
        "\n",
        "  else:\n",
        "    Selected_path =  \"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(pkl files)/\" + \"Single Lingual\"\n",
        "\n",
        "    if Dataset_code == 0:\n",
        "      first_path = os.path.join(Selected_path, \"French_\" + \"prob_per_10epochs\" + \".pkl\")\n",
        "    elif Dataset_code == 1:\n",
        "      first_path = os.path.join(Selected_path, \"English_\" + \"prob_per_10epochs\" + \".pkl\")\n",
        "    else:\n",
        "      first_path = os.path.join(Selected_path, \"Farsi_\" + \"prob_per_10epochs\" + \".pkl\")\n",
        "\n",
        "    with open(first_path, 'wb') as f:\n",
        "\n",
        "      pickle.dump(prob_first_task, f)"
      ],
      "metadata": {
        "id": "xLD4o2zeyMd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batch(multitask_model, features_dict, trainer, tokenizer, max_length, val_len, batch_size, task_name):\n",
        "  acc = 0\n",
        "  multitask_model.taskmodels_dict[task_name].eval()\n",
        "  with torch.no_grad():\n",
        "    for index in range(0, val_len, batch_size):\n",
        "        batch = features_dict[task_name][\"test\"][\"utt\"][index : min(index + batch_size, val_len)]\n",
        "\n",
        "        labels = features_dict[task_name][\"test\"][\"intent\"][index : min(index + batch_size, val_len)]\n",
        "        inputs = tokenizer.batch_encode_plus(batch, max_length=max_length, pad_to_max_length=True, return_tensors='pt').to(trainer.args.device)\n",
        "        logits = multitask_model(task_name, **inputs)[0]\n",
        "        predictions = torch.argmax(torch.FloatTensor(torch.softmax(logits, dim=1).detach().cpu().tolist()), dim=1)\n",
        "        acc += sum(np.array(predictions) == np.array(labels))\n",
        "\n",
        "  return acc"
      ],
      "metadata": {
        "id": "0sllk-upYmzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multitask_eval_fn(multitask_model, features_dict, model_name, dataset_dict, trainer, tokenizer, max_length, batch_size, eval_dataset = 'test'):\n",
        "    preds_dict = {}\n",
        "    for task_name in list(dataset_dict.keys()):\n",
        "        val_len = len(features_dict[task_name][eval_dataset])\n",
        "        acc_per_task = create_batch(multitask_model,\n",
        "                                    features_dict,\n",
        "                                    trainer,\n",
        "                                    tokenizer,\n",
        "                                    max_length,\n",
        "                                    val_len,\n",
        "                                    batch_size,\n",
        "                                    task_name)\n",
        "\n",
        "        acc_per_task= acc_per_task / val_len\n",
        "        print(\"Language: %s \\t Accuracy: %.4f\" % (task_name, acc_per_task))"
      ],
      "metadata": {
        "id": "djFNIRu-VjsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def return_features(dataset_code, task_num = 1):\n",
        "\n",
        "#   '''\n",
        "#   task_num is the shortend form of task number which specifies the first/second task as an integer number(1 for the first task and 2 for the second task)\n",
        "#   '''\n",
        "#   lst = [Features_list[dataset_code[0]], Features_list[dataset_code[1]]]\n",
        "#   return lst[task_num - 1]\n"
      ],
      "metadata": {
        "id": "8eDw8UPDPAtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_forgettable_samples(prob_arr, dataset, dataset_code, Fine_tune):\n",
        "\n",
        "  true_label = dataset['train']['intent']\n",
        "\n",
        "\n",
        "  forgettable_info = {'sample_index':[],'forgetting_events':[],'Match_label_indexs':[]}\n",
        "  for i in range(prob_arr.shape[1]):\n",
        "    forgetting_events = 0\n",
        "    prob_per_sample = prob_arr[:,i,:]\n",
        "    label_per_sample = np.argmax(prob_per_sample,axis=1)\n",
        "    Match_label_indexs = np.where(label_per_sample == true_label[i])[0]\n",
        "\n",
        "    if Match_label_indexs.shape[0] == 0: # no correct prediction\n",
        "      forgettable_info['sample_index'].append((i if Fine_tune == 0 or isinstance(dataset_code, int) else dataset['train']['index'][i]))\n",
        "      forgettable_info['forgetting_events'].append(0)\n",
        "      forgettable_info['Match_label_indexs'].append(Match_label_indexs)\n",
        "\n",
        "    else:\n",
        "      for item in (Match_label_indexs + 1):\n",
        "        if item not in Match_label_indexs and item < prob_arr.shape[0]:\n",
        "            forgetting_events += 1\n",
        "      if forgetting_events > 0:\n",
        "        forgettable_info['sample_index'].append((i if Fine_tune == 0 or isinstance(dataset_code, int) else dataset['train']['index'][i]))\n",
        "        forgettable_info['forgetting_events'].append(forgetting_events)\n",
        "        forgettable_info['Match_label_indexs'].append(Match_label_indexs)\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "  return forgettable_info\n",
        ""
      ],
      "metadata": {
        "id": "sK5EyX64mpCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_dataset(Dataset_code, Combination, Fine_tune = 1, drop_unlearned = 0):\n",
        "\n",
        "  '''\n",
        "  |----------------------------------------------------------------------------------------------------------------------|\n",
        "  |dataset_code is an indication of each of the available tasks (datasets).                                              |\n",
        "  |dataset_code = 0: the corresponding French task/dataset,                                                           |\n",
        "  |dataset_code = 1: the corresponding English task/dataset,                                                          |\n",
        "  |dataset_code = 2: the corresponding Farsi task/dataset.                                                            |\n",
        "  |----------------------------------------------------------------------------------------------------------------------|\n",
        "  |Combination is an integer number ranging from 0 to 2, pointing out to one of the possible options in multi-lingual mode. |\n",
        "  |Combination = 0: We are considering => English & French,                                                              |\n",
        "  |Combination = 1: We are considering => English & Farsi,                                                               |\n",
        "  |Combination = 2: We are considering => French & Farsi.                                                                |\n",
        "  |----------------------------------------------------------------------------------------------------------------------|\n",
        "  |If Fine-tune = 1 => Load the forgettable examples only dataset,                                                       |\n",
        "  |If Fine-tune = 0 => Load the initial dataset                                                                          |\n",
        "  |----------------------------------------------------------------------------------------------------------------------|\n",
        "  '''\n",
        "  if isinstance(Dataset_code, list):\n",
        "\n",
        "    if Fine_tune:\n",
        "        dataset_dict = load_dict()\n",
        "        # Checking Modes\n",
        "        if Combination == 0:\n",
        "            #   df_multi_E_EP = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(csv files)/Multi Lingual/English_Forgettables_Info_FREN.csv\")\n",
        "            df_multi_E_EP = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(pkl files)/Multi Lingual/English_MLL_prob_per_10epochs_FREN.csv\")\n",
        "            #   df_multi_P_EP = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(csv files)/Multi Lingual/French_Forgettables_Info_FREN.csv\")\n",
        "            df_multi_P_EP = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(pkl files)/Multi Lingual/French_MLL_prob_per_10epochs_FREN.csv\")\n",
        "\n",
        "            # droping unlearned samples\n",
        "            if (drop_unlearned):\n",
        "                df_multi_E_EP = df_multi_E_EP[df_multi_E_EP['Match_label_indexs'] != '[]']\n",
        "                df_multi_P_EP = df_multi_P_EP[df_multi_P_EP['Match_label_indexs'] != '[]']\n",
        "\n",
        "            # Configs for English\n",
        "            df_train_Entailment = pd.DataFrame(dataset_dict[\"English\"]['train'])\n",
        "            #   df_train_Entailment['label'] = (df_train_Entailment['label'] == 'entails').astype(int)\n",
        "            df_train_Entailment['index'] = pd.Series(range(len(df_train_Entailment)))\n",
        "            df_train_Entailment = df_train_Entailment[(pd.concat([df_train_Entailment['index'], df_multi_E_EP['sample_index']]).value_counts() == 2)]\n",
        "            df_test_Entailment = pd.DataFrame(dataset_dict[\"English\"]['test'])\n",
        "            #   df_test_Entailment['label'] = (df_test_Entailment['label'] == 'entails').astype(int)\n",
        "            df_validation_Entailment =  pd.DataFrame(dataset_dict[\"English\"]['validation'])\n",
        "            #   df_validation_Entailment ['label'] = (df_validation_Entailment ['label'] == 'entails').astype(int)\n",
        "\n",
        "            dataset_dict[\"English\"]['train'] = Dataset.from_pandas(df_train_Entailment, preserve_index=False)\n",
        "            dataset_dict[\"English\"]['test'] = Dataset.from_pandas(df_test_Entailment, preserve_index=False)\n",
        "            dataset_dict[\"English\"]['validation'] = Dataset.from_pandas(df_validation_Entailment, preserve_index=False)\n",
        "            # Configs for French\n",
        "            df_train_Paraphrase = pd.DataFrame(dataset_dict[\"French\"]['train'])\n",
        "            df_train_Paraphrase['index'] = pd.Series(range(len(df_train_Paraphrase)))\n",
        "            df_train_Paraphrase = df_train_Paraphrase[(pd.concat([df_train_Paraphrase['index'],df_multi_P_EP['sample_index']]).value_counts() == 2)]\n",
        "            df_test_Paraphrase = pd.DataFrame(dataset_dict[\"French\"]['test'])\n",
        "            df_validation_Paraphrase = pd.DataFrame(dataset_dict[\"French\"]['validation'])\n",
        "\n",
        "            dataset_dict[\"French\"]['train'] = Dataset.from_pandas(df_train_Paraphrase, preserve_index=False)\n",
        "            dataset_dict[\"French\"]['test'] = Dataset.from_pandas(df_test_Paraphrase, preserve_index=False)\n",
        "            dataset_dict[\"French\"]['validation'] = Dataset.from_pandas(df_validation_Paraphrase, preserve_index=False)\n",
        "            # return the dataset_dict for the specified combination\n",
        "            my_dict = {}\n",
        "            my_dict[\"English\"] = dataset_dict[\"English\"]\n",
        "            my_dict[\"French\"] = dataset_dict[\"French\"]\n",
        "\n",
        "            return my_dict\n",
        "\n",
        "\n",
        "        elif Combination == 1:\n",
        "            #   df_multi_S_ES = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(csv files)/Multi Lingual/Farsi_Forgettables_Info_ENFA.csv\")\n",
        "            df_multi_S_ES = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(pkl files)/Multi Lingual/Farsi_MLL_prob_per_10epochs_ENFA.csv\")\n",
        "            #   df_multi_E_ES = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(csv files)/Multi Lingual/English_Forgettables_Info_ENFA.csv\")\n",
        "            df_multi_E_ES = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(pkl files)/Multi Lingual/English_MLL_prob_per_10epochs_ENFA.csv\")\n",
        "\n",
        "            # droping unlearned samples\n",
        "            if (drop_unlearned):\n",
        "                df_multi_S_ES = df_multi_S_ES[df_multi_S_ES['Match_label_indexs'] != '[]']\n",
        "                df_multi_E_ES = df_multi_E_ES[df_multi_E_ES['Match_label_indexs'] != '[]']\n",
        "\n",
        "            # Configs for English\n",
        "            df_train_Entailment = pd.DataFrame(dataset_dict[\"English\"]['train'])\n",
        "            #   df_train_Entailment['label'] = (df_train_Entailment['label'] == 'entails').astype(int)\n",
        "            df_train_Entailment['index'] = pd.Series(range(len(df_train_Entailment)))\n",
        "            df_train_Entailment = df_train_Entailment[(pd.concat([df_train_Entailment['index'],df_multi_E_ES['sample_index']]).value_counts() == 2)]\n",
        "            df_test_Entailment = pd.DataFrame(dataset_dict[\"English\"]['test'])\n",
        "            #   df_test_Entailment['label'] = (df_test_Entailment['label'] == 'entails').astype(int)\n",
        "            df_validation_Entailment =  pd.DataFrame(dataset_dict[\"English\"]['validation'])\n",
        "            #   df_validation_Entailment ['label'] = (df_validation_Entailment ['label'] == 'entails').astype(int)\n",
        "\n",
        "            dataset_dict[\"English\"]['train'] = Dataset.from_pandas(df_train_Entailment, preserve_index=False)\n",
        "            dataset_dict[\"English\"]['test'] = Dataset.from_pandas(df_test_Entailment, preserve_index=False)\n",
        "            dataset_dict[\"English\"]['validation'] = Dataset.from_pandas(df_validation_Entailment, preserve_index=False)\n",
        "            # Configs for Farsi\n",
        "            df_train_Sentiment = pd.DataFrame(dataset_dict[\"Farsi\"]['train'])\n",
        "            df_train_Sentiment['index'] = pd.Series(range(len(df_train_Sentiment)))\n",
        "            df_train_Sentiment = df_train_Sentiment[(pd.concat([df_train_Sentiment['index'],df_multi_S_ES['sample_index']]).value_counts() == 2)]\n",
        "            df_test_Sentiment = pd.DataFrame(dataset_dict[\"Farsi\"]['test'])\n",
        "            df_validation_Sentiment =  pd.DataFrame(dataset_dict[\"Farsi\"]['validation'])\n",
        "\n",
        "            dataset_dict[\"Farsi\"]['train'] = Dataset.from_pandas(df_train_Sentiment, preserve_index=False)\n",
        "            dataset_dict[\"Farsi\"]['test'] = Dataset.from_pandas(df_test_Sentiment, preserve_index=False)\n",
        "            dataset_dict[\"Farsi\"]['validation'] = Dataset.from_pandas(df_validation_Sentiment, preserve_index=False)\n",
        "            # return the dataset_dict for the specified combination\n",
        "            my_dict = {}\n",
        "            my_dict[\"English\"] = dataset_dict[\"English\"]\n",
        "            my_dict[\"Farsi\"] = dataset_dict[\"Farsi\"]\n",
        "\n",
        "            return my_dict\n",
        "\n",
        "\n",
        "        elif Combination == 2:\n",
        "            #   df_multi_P_PS = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(csv files)/Multi Lingual/French_Forgettables_Info_FRFA.csv\")\n",
        "            df_multi_P_PS = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(pkl files)/Multi Lingual/French_MLL_prob_per_10epochs_FRFA.csv\")\n",
        "            #   df_multi_S_PS = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(csv files)/Multi Lingual/Farsi_Forgettables_Info_FRFA.csv\")\n",
        "            df_multi_S_PS = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(pkl files)/Multi Lingual/Farsi_MLL_prob_per_10epochs_FRFA.csv\")\n",
        "\n",
        "            # droping unlearned samples\n",
        "            if (drop_unlearned):\n",
        "                df_multi_P_PS = df_multi_P_PS[df_multi_P_PS['Match_label_indexs'] != '[]']\n",
        "                df_multi_S_PS = df_multi_S_PS[df_multi_S_PS['Match_label_indexs'] != '[]']\n",
        "\n",
        "            # Configs for French\n",
        "            df_train_Paraphrase = pd.DataFrame(dataset_dict[\"French\"]['train'])\n",
        "            df_train_Paraphrase['index'] = pd.Series(range(len(df_train_Paraphrase)))\n",
        "            df_train_Paraphrase = df_train_Paraphrase[(pd.concat([df_train_Paraphrase['index'],df_multi_P_PS['sample_index']]).value_counts() == 2)]\n",
        "            df_test_Paraphrase = pd.DataFrame(dataset_dict[\"French\"]['test'])\n",
        "            df_validation_Paraphrase =  pd.DataFrame(dataset_dict[\"French\"]['validation'])\n",
        "\n",
        "            dataset_dict[\"French\"]['train'] = Dataset.from_pandas(df_train_Paraphrase, preserve_index=False)\n",
        "            dataset_dict[\"French\"]['test'] = Dataset.from_pandas(df_test_Paraphrase, preserve_index=False)\n",
        "            dataset_dict[\"French\"]['validation'] = Dataset.from_pandas(df_validation_Paraphrase, preserve_index=False)\n",
        "            # Configs for Farsi\n",
        "            df_train_Sentiment = pd.DataFrame(dataset_dict[\"Farsi\"]['train'])\n",
        "            df_train_Sentiment['index'] = pd.Series(range(len(df_train_Sentiment)))\n",
        "            df_train_Sentiment = df_train_Sentiment[(pd.concat([df_train_Sentiment['index'],df_multi_S_PS['sample_index']]).value_counts() == 2)]\n",
        "            df_test_Sentiment = pd.DataFrame(dataset_dict[\"Farsi\"]['test'])\n",
        "            df_validation_Sentiment =  pd.DataFrame(dataset_dict[\"Farsi\"]['validation'])\n",
        "\n",
        "            dataset_dict[\"Farsi\"]['train'] = Dataset.from_pandas(df_train_Sentiment, preserve_index=False)\n",
        "            dataset_dict[\"Farsi\"]['test'] = Dataset.from_pandas(df_test_Sentiment, preserve_index=False)\n",
        "            dataset_dict[\"Farsi\"]['validation'] = Dataset.from_pandas(df_validation_Sentiment, preserve_index=False)\n",
        "            # return the dataset_dict for the specified combination\n",
        "            my_dict = {}\n",
        "            my_dict[\"French\"] = dataset_dict[\"French\"]\n",
        "            my_dict[\"Farsi\"] = dataset_dict[\"Farsi\"]\n",
        "\n",
        "            return my_dict\n",
        "\n",
        "    else:\n",
        "        dataset_dict = load_dict()\n",
        "        # Checking modes\n",
        "        if Combination == 0:\n",
        "            return {\"English\": dataset_dict['English'],\"French\": dataset_dict[\"French\"]}\n",
        "        elif Combination == 1:\n",
        "            return {\"English\": dataset_dict['English'], \"Farsi\": dataset_dict[\"Farsi\"]}\n",
        "        elif Combination == 2:\n",
        "            return {\"French\": dataset_dict[\"French\"], \"Farsi\": dataset_dict[\"Farsi\"]}\n",
        "\n",
        "  else:\n",
        "    dataset_dict = load_dict()\n",
        "\n",
        "    if (Fine_tune):\n",
        "        if (Dataset_code == 0):\n",
        "            df_single = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(pkl files)/Single Lingual/French_FRG_Info.csv\")\n",
        "\n",
        "            # droping unlearned samples\n",
        "            if (drop_unlearned):\n",
        "                df_single = df_single[df_single['Match_label_indexs'] != '[]']\n",
        "\n",
        "            # Configs for French\n",
        "            df_train_Entailment = pd.DataFrame(dataset_dict[\"English\"]['train'])\n",
        "            df_train_Entailment['index'] = pd.Series(range(len(df_train_Entailment)))\n",
        "            df_train_Entailment = df_train_Entailment[(pd.concat([df_train_Entailment['index'], df_single['sample_index']]).value_counts() == 2)]\n",
        "            df_test_Entailment = pd.DataFrame(dataset_dict[\"English\"]['test'])\n",
        "            df_validation_Entailment =  pd.DataFrame(dataset_dict[\"English\"]['validation'])\n",
        "\n",
        "            dataset_dict[\"French\"]['train'] = Dataset.from_pandas(df_train_Entailment, preserve_index=False)\n",
        "            dataset_dict[\"French\"]['test'] = Dataset.from_pandas(df_test_Entailment, preserve_index=False)\n",
        "            dataset_dict[\"French\"]['validation'] = Dataset.from_pandas(df_validation_Entailment, preserve_index=False)\n",
        "\n",
        "            my_dict = {}\n",
        "            my_dict[\"French\"] = dataset_dict[\"French\"]\n",
        "            return my_dict\n",
        "\n",
        "        elif (Dataset_code == 1):\n",
        "            df_single = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(pkl files)/Single Lingual/English_FRG_Info.csv\")\n",
        "\n",
        "            # droping unlearned samples\n",
        "            if (drop_unlearned):\n",
        "                df_single = df_single[df_single['Match_label_indexs'] != '[]']\n",
        "\n",
        "            # Configs for English\n",
        "            df_train_Entailment = pd.DataFrame(dataset_dict[\"English\"]['train'])\n",
        "            df_train_Entailment['index'] = pd.Series(range(len(df_train_Entailment)))\n",
        "            df_train_Entailment = df_train_Entailment[(pd.concat([df_train_Entailment['index'], df_single['sample_index']]).value_counts() == 2)]\n",
        "            df_test_Entailment = pd.DataFrame(dataset_dict[\"English\"]['test'])\n",
        "            df_validation_Entailment =  pd.DataFrame(dataset_dict[\"English\"]['validation'])\n",
        "\n",
        "            dataset_dict[\"English\"]['train'] = Dataset.from_pandas(df_train_Entailment, preserve_index=False)\n",
        "            dataset_dict[\"English\"]['test'] = Dataset.from_pandas(df_test_Entailment, preserve_index=False)\n",
        "            dataset_dict[\"English\"]['validation'] = Dataset.from_pandas(df_validation_Entailment, preserve_index=False)\n",
        "\n",
        "            my_dict = {}\n",
        "            my_dict[\"English\"] = dataset_dict[\"English\"]\n",
        "\n",
        "            return my_dict\n",
        "        else:\n",
        "            df_single = pd.read_csv(\"/content/drive/MyDrive/NLP Bachelors' Project/FRG_Info(pkl files)/Single Lingual/Farsi_FRG_Info.csv\")\n",
        "\n",
        "            # droping unlearned samples\n",
        "            if (drop_unlearned):\n",
        "                df_single = df_single[df_single['Match_label_indexs'] != '[]']\n",
        "\n",
        "            # Configs for English\n",
        "            df_train_Entailment = pd.DataFrame(dataset_dict[\"Farsi\"]['train'])\n",
        "            df_train_Entailment['index'] = pd.Series(range(len(df_train_Entailment)))\n",
        "            df_train_Entailment = df_train_Entailment[(pd.concat([df_train_Entailment['index'], df_single['sample_index']]).value_counts() == 2)]\n",
        "            df_test_Entailment = pd.DataFrame(dataset_dict[\"Farsi\"]['test'])\n",
        "            df_validation_Entailment =  pd.DataFrame(dataset_dict[\"Farsi\"]['validation'])\n",
        "\n",
        "            dataset_dict[\"Farsi\"]['train'] = Dataset.from_pandas(df_train_Entailment, preserve_index=False)\n",
        "            dataset_dict[\"Farsi\"]['test'] = Dataset.from_pandas(df_test_Entailment, preserve_index=False)\n",
        "            dataset_dict[\"Farsi\"]['validation'] = Dataset.from_pandas(df_validation_Entailment, preserve_index=False)\n",
        "\n",
        "            my_dict = {}\n",
        "            my_dict[\"Farsi\"] = dataset_dict[\"Farsi\"]\n",
        "\n",
        "            return my_dict\n",
        "\n",
        "\n",
        "    else:\n",
        "        if Dataset_code == 0:\n",
        "            return {\"French\": dataset_dict[\"French\"]}\n",
        "\n",
        "        elif Dataset_code == 1:\n",
        "            return {\"English\": dataset_dict[\"English\"]}\n",
        "\n",
        "        else:\n",
        "            return {\"Farsi\": dataset_dict[\"Farsi\"]}\n"
      ],
      "metadata": {
        "id": "ib4sbXzWEJ76"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}